{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexical normalization pipeline \n",
    "\n",
    "author - AR Dirkson \n",
    "date - 2-10-2018\n",
    "\n",
    "This pipeline takes raw text data and performs: \n",
    "- Removes URLs, email addresses and personal pronouns\n",
    "- Convert to lower-case\n",
    "- Tokenization with NLTK\n",
    "- Removes non_English posts (conservatively) using langid module with top 10 languages and threshold of 100\n",
    "- British English to American English \n",
    "- Normalization of generic abbreviations and slang \n",
    "- Normalization of domain-specific (patient forum) abbreviations \n",
    "- Spelling correction \n",
    "\n",
    "The spelling correction only works on a Linux platform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the relevant objects should be in obj_lex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you have not already done so you will need to import the names corpus\n",
    "\n",
    "import nltk\n",
    "nltk.download('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pickle \n",
    "import re\n",
    "import numpy as np \n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import editdistance\n",
    "import csv \n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import scipy.stats \n",
    "from nltk.corpus import names\n",
    "\n",
    "from weighted_levenshtein import lev, osa, dam_lev\n",
    "\n",
    "import langid\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer (): \n",
    "        \n",
    "    def __init__(self): \n",
    "        pass\n",
    "        \n",
    "    #to use this function the files need to be sorted in the same folder as the script under /obj_lex/\n",
    "    def load_obj(self, name):\n",
    "        with open('obj_lex/' + name + '.pkl', 'rb') as f:\n",
    "            return pickle.load(f, encoding='latin1')\n",
    "        \n",
    "    def load_files(self): \n",
    "        self.ext_vocab2 = self.load_obj('vocabulary_spelling_unique')\n",
    "        self.abbr_dict = self.load_obj ('abbreviations_dict')\n",
    "        self.celex_freq_dict = self.load_obj ('celex_lwrd_frequencies')\n",
    "        self.celex_list = list(self.celex_freq_dict.keys())\n",
    "        self.celex_set = set (self.celex_list)\n",
    "        self.drug_norm_dict = self.load_obj ('drug_normalize_dict')\n",
    "\n",
    "    def change_tup_to_list(self, tup): \n",
    "        thelist = list(tup)\n",
    "        return thelist\n",
    "    \n",
    "    def change_list_to_tup(self,thelist): \n",
    "        tup = tuple(thelist)\n",
    "        return tup\n",
    "    \n",
    "#---------Remove URls, email addresses and personal pronouns ------------------\n",
    "        \n",
    "    def replace_urls(self,list_of_msgs): \n",
    "        list_of_msgs2 = []\n",
    "        for msg in list_of_msgs: \n",
    "            nw_msg = re.sub(\n",
    "        r'\\b' + r'((\\(<{0,1}https|\\(<{0,1}http|\\[<{0,1}https|\\[<{0,1}http|<{0,1}https|<{0,1}http)(:|;| |: )\\/\\/|www.)[\\w\\.\\/#\\?\\=\\+\\;\\,\\&\\%_\\n-]+(\\.[a-z]{2,4}\\]{0,1}\\){0,1}|\\.html\\]{0,1}\\){0,1}|\\/[\\w\\.\\?\\=#\\+\\;\\,\\&\\%_-]+|[\\w\\/\\.\\?\\=#\\+\\;\\,\\&\\%_-]+|[0-9]+#m[0-9]+)+(\\n|\\b|\\s|\\/|\\]|\\)|>)',\n",
    "        '-URL-', msg)\n",
    "            list_of_msgs2.append(nw_msg)\n",
    "        return list_of_msgs2    \n",
    "\n",
    "    def replace_email(self,list_of_msgs): \n",
    "        list_of_msgs2 = []\n",
    "        for msg in list_of_msgs: \n",
    "            nw_msg = re.sub (r\"([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)\", '-EMAIL-', msg) \n",
    "            list_of_msgs2.append(nw_msg)\n",
    "        return list_of_msgs2\n",
    "\n",
    "    def remove_empty (self,list_of_msgs): \n",
    "        empty = []\n",
    "        check_msgs3 =[]\n",
    "        for a, i in enumerate (list_of_msgs): \n",
    "            if len(i) == 0: \n",
    "                print('empty')\n",
    "            else: \n",
    "                check_msgs3.append(i)\n",
    "        return check_msgs3\n",
    "    \n",
    "\n",
    "    def create_names_list (self): \n",
    "        male_names = names.words('male.txt')\n",
    "        female_names = names.words('female.txt')\n",
    "        male_set = set (male_names)\n",
    "        female_set = set (female_names)\n",
    "        names_set = male_set.union(female_set) \n",
    "        names_list = [] \n",
    "        for word in names_set: \n",
    "            if (word != 'ned') & (word != 'Ned'): #ned means no evidence and is an important medical term\n",
    "                word1 = str.lower (word)\n",
    "                names_list.append(word1) #add the lowered words\n",
    "                names_list.append(word) #add the capitalized words\n",
    "        \n",
    "        self.names_list = names_list\n",
    "    \n",
    "    def remove_propernoun_names(self,msg):\n",
    "        try: \n",
    "            nw_msg = [self.change_tup_to_list(token) for token in msg]\n",
    "            for a, token in enumerate (nw_msg):\n",
    "                if (token[0] in self.names_list) and ((token[1] == 'NNP') or (token[1]== 'NNPS')): \n",
    "                    new_token = token[0].replace (token[0], \"-NAME-\")\n",
    "                    nw_msg[a] = [new_token, token[1]]\n",
    "#             nw_msg2 = [self.change_list_to_tup(token) for token in nw_msg]\n",
    "            return nw_msg\n",
    "        except TypeError: \n",
    "            pass\n",
    "    \n",
    "    \n",
    "    def anonymize (self, posts): \n",
    "        posts2 = self.replace_urls (posts)\n",
    "        posts3 = self.replace_email (posts2)\n",
    "        posts4 = self.remove_empty(posts3)\n",
    "        posts5 = [word_tokenize (sent) for sent in posts4]\n",
    "        posts6 = [pos_tag(sent) for sent in posts5]\n",
    "        self.create_names_list()\n",
    "        posts7 = [self.remove_propernoun_names (m) for m in posts6]\n",
    "        posts8 = []\n",
    "        for post in posts7: \n",
    "            tg = [m[0] for m in post]\n",
    "            posts8.append(tg)\n",
    "        return posts8\n",
    "\n",
    "#---------Convert to lowercase ----------------------------------------------------\n",
    "    \n",
    "    def lowercase (self, post):\n",
    "        post1 = []\n",
    "        for word in post: \n",
    "            word1 = word.lower()\n",
    "            post1.append (word1)\n",
    "        return post1\n",
    "\n",
    "#---------Remove non_English posts -------------------------------------------------    \n",
    "    def language_identify_basic (self, posts):\n",
    "        nw = []\n",
    "        tally = 0\n",
    "        list_removed = []\n",
    "        for post in posts: \n",
    "            out = langid.classify (post)\n",
    "            out2 = list(out)\n",
    "            if out2[0]=='en': \n",
    "                nw.append(post)\n",
    "            else: \n",
    "                tally += 1 \n",
    "                list_removed.append(tuple ([post, out2[0], out2[1]]))\n",
    "        return nw, tally, list_removed\n",
    "    \n",
    "    def language_identify_thres (self, msgs, lang_list, thres):\n",
    "        nw = []\n",
    "        tally = 0\n",
    "        list_removed = []\n",
    "        for post in msgs: \n",
    "            langid.set_languages(lang_list)\n",
    "            out = langid.classify (post)\n",
    "            out2 = list(out)\n",
    "            if out2[0]=='en': \n",
    "                nw.append(post)\n",
    "            elif out2[1] > thres:\n",
    "                nw.append(post)\n",
    "            else: \n",
    "                tally += 1 \n",
    "                list_removed.append(tuple ([post, out2[0], out2[1]]))\n",
    "        return nw, tally, list_removed   \n",
    "\n",
    "    \n",
    "    def remove_non_english(self, posts): \n",
    "        d = TreebankWordDetokenizer()\n",
    "        posts2 = [d.detokenize(m) for m in posts]\n",
    "        \n",
    "        posts_temp, tally, list_removed = self.language_identify_basic(posts2)        \n",
    "        lang = []\n",
    "\n",
    "        for itm in list_removed: \n",
    "            lang.append(itm[1])\n",
    "\n",
    "        c = Counter(lang)\n",
    "\n",
    "        lang_list = ['en']\n",
    "\n",
    "        for itm in c.most_common(10): \n",
    "            z = list(itm)\n",
    "            lang_list.append(z[0])\n",
    "    \n",
    "        print(\"Most common 10 languages in the data are:\" + str(lang_list))\n",
    "        posts3, tally_nw, list_removed_nw = self.language_identify_thres(posts2, lang_list, thres = -100)\n",
    "        return posts3\n",
    "    \n",
    "#---------Lexical normalization pipeline (Sarker, 2017) -------------------------------\n",
    "\n",
    "    def loadItems(self):\n",
    "        '''\n",
    "        This is the primary load function.. calls other loader functions as required..\n",
    "        '''    \n",
    "        global english_to_american\n",
    "        global noslang_dict\n",
    "        global IGNORE_LIST_TRAIN\n",
    "        global IGNORE_LIST\n",
    "\n",
    "        english_to_american = {}\n",
    "        lexnorm_oovs = []\n",
    "        IGNORE_LIST_TRAIN = []\n",
    "        IGNORE_LIST = []\n",
    "\n",
    "        english_to_american = self.loadEnglishToAmericanDict()\n",
    "        noslang_dict = self.loadDictionaryData()\n",
    "        for key, value in noslang_dict.items (): \n",
    "            value2 = value.lower ()\n",
    "            value3 = word_tokenize (value2)\n",
    "            noslang_dict[key] = value3\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    def loadEnglishToAmericanDict(self):\n",
    "        etoa = {}\n",
    "\n",
    "        english = open('obj_lex/englishspellings.txt')\n",
    "        american = open('obj_lex/americanspellings.txt')\n",
    "        for line in english:\n",
    "            etoa[line.strip()] = american.readline().strip()\n",
    "        return etoa\n",
    "\n",
    "    def loadDictionaryData(self):\n",
    "        '''\n",
    "        this function loads the various dictionaries which can be used for mapping from oov to iv\n",
    "        '''\n",
    "        n_dict = {}\n",
    "        infile = open('obj_lex/noslang_mod.txt')\n",
    "        for line in infile:\n",
    "            items = line.split(' - ')\n",
    "            if len(items[0]) > 0 and len(items) > 1:\n",
    "                n_dict[items[0].strip()] = items[1].strip()\n",
    "        return n_dict\n",
    "\n",
    "\n",
    "\n",
    "    def preprocessText(self, tokens, IGNORE_LIST, ignore_username=False, ignore_hashtag=False, ignore_repeated_chars=True, eng_to_am=True, ignore_urls=False):\n",
    "        '''\n",
    "        Note the reason it ignores hashtags, @ etc. is because there is a preprocessing technique that is \n",
    "            designed to remove them \n",
    "        '''\n",
    "        normalized_tokens =[]\n",
    "        #print tokens\n",
    "        text_string = ''\n",
    "        # NOTE: if nesting if/else statements, be careful about execution sequence...\n",
    "        for t in tokens:\n",
    "            t_lower = t.strip().lower()\n",
    "            # if the token is not in the IGNORE_LIST, do various transformations (e.g., ignore usernames and hashtags, english to american conversion\n",
    "            # and others..\n",
    "            if t_lower not in IGNORE_LIST:\n",
    "                # ignore usernames '@'\n",
    "                if re.match('@', t) and ignore_username:\n",
    "                    IGNORE_LIST.append(t_lower)\n",
    "                    text_string += t_lower + ' '\n",
    "                #ignore hashtags\n",
    "                elif re.match('#', t_lower) and ignore_hashtag:\n",
    "                    IGNORE_LIST.append(t_lower)\n",
    "                    text_string += t_lower + ' '\n",
    "                #convert english spelling to american spelling\n",
    "                elif t.strip().lower() in english_to_american.keys() and eng_to_am:    \n",
    "                    text_string += english_to_american[t.strip().lower()] + ' '\n",
    "                #URLS\n",
    "                elif re.search('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', t_lower) and ignore_urls:\n",
    "                    IGNORE_LIST.append(t_lower)\n",
    "                    text_string += t_lower + ' '                \n",
    "                elif not ignore_repeated_chars and not re.search(r'[^a-zA-Z]', t_lower):\n",
    "                    # if t_lower only contains alphabetic characters\n",
    "                    t_lower = re.sub(r'([a-z])\\1+', r'\\1\\1', t_lower)\n",
    "                    text_string += t_lower + ' '  \n",
    "                    # print t_lower\n",
    "\n",
    "                # if none of the conditions match, just add the token without any changes..\n",
    "                else:\n",
    "                    text_string += t_lower + ' '\n",
    "            else:  # i.e., if the token is in the ignorelist..\n",
    "                text_string += t_lower + ' '\n",
    "            normalized_tokens = text_string.split()\n",
    "        # print normalized_tokens\n",
    "        return normalized_tokens, IGNORE_LIST\n",
    "\n",
    "\n",
    "    def dictionaryBasedNormalization(self, tokens, I_LIST, M_LIST):\n",
    "        tokens2 =[]\n",
    "        for t in (tokens):\n",
    "            t_lower = t.strip().lower()\n",
    "            if t_lower in noslang_dict.keys() and len(t_lower)>2:\n",
    "                nt = noslang_dict[t_lower]\n",
    "                [tokens2.append(m) for m in nt]\n",
    "\n",
    "                if not t_lower in M_LIST:\n",
    "                    M_LIST.append(t_lower)\n",
    "                if not nt in M_LIST:\n",
    "                    M_LIST.append(nt)\n",
    "            else: \n",
    "                tokens2.append (t)\n",
    "        return tokens2, I_LIST, M_LIST\n",
    "    \n",
    "#----Using the Sarker normalization functions ----------------------------\n",
    "#Step 1 is the English normalization and step 2 is the abbreviation normalization\n",
    "\n",
    "    def normalize_step1(self, tokens, oovoutfile=None):\n",
    "        global IGNORE_LIST\n",
    "        global il\n",
    "        MOD_LIST = []\n",
    "        # Step 1: preprocess the text\n",
    "        normalized_tokens, il = self.preprocessText(tokens, IGNORE_LIST)\n",
    "        return normalized_tokens\n",
    "    \n",
    "    def normalize_step2(self, normalized_tokens, oovoutfile=None): \n",
    "        global IGNORE_LIST\n",
    "        global il\n",
    "        MOD_LIST = []    \n",
    "        ml = MOD_LIST\n",
    "        normalized_tokens, il, ml = self.dictionaryBasedNormalization(normalized_tokens, il, ml)\n",
    "        return normalized_tokens\n",
    "\n",
    "    def sarker_normalize (self,list_of_msgs): \n",
    "        self.loadItems()\n",
    "        msgs_normalized = [self.normalize_step1(m) for m in list_of_msgs]\n",
    "        msgs_normalized2 = [self.normalize_step2(m) for m in msgs_normalized]    \n",
    "        return msgs_normalized2\n",
    "\n",
    "#-------Domain specific abreviation expansion ----------------------------\n",
    "# The list of abbreviations is input as a dictionary with tokenized output  \n",
    "\n",
    "    def domain_specific_abbr (self, tokens, abbr): \n",
    "        post2 = [] \n",
    "        for t in tokens:\n",
    "            if t in abbr.keys(): \n",
    "                nt = abbr[t]\n",
    "                [post2.append(m) for m in nt]\n",
    "            else: \n",
    "                post2.append(t)\n",
    "        return post2\n",
    "\n",
    "    def expand_abbr (self, data, abbr): \n",
    "        data2 = []\n",
    "        for post in data: \n",
    "            post2 = self.domain_specific_abbr (tokens = post, abbr= abbr)\n",
    "            data2.append(post2)\n",
    "        return data2\n",
    "    \n",
    "#-------Spelling correction -------------------------------------------------    \n",
    "    \n",
    "    def load_files2 (self): \n",
    "        #load the edit matrices\n",
    "        #transpositions\n",
    "        self.edits_trans = self.load_obj ('weighted_edits_transpositions')\n",
    "        #deletions \n",
    "        self.edits_del = self.load_obj('weighted_edits_deletions')\n",
    "        #insertions \n",
    "        self.edits_ins = self.load_obj('weighted_edits_insertions')\n",
    "        #substitutions\n",
    "        self.edits_sub = self.load_obj('weighted_edits_substitutions')\n",
    "                \n",
    "        #load the generic dictionary - CHANGE PATH!  \n",
    "        self.celex_freq_dict = self.load_obj ('celex_lwrd_frequencies')\n",
    "    \n",
    "    \n",
    "    def initialize_weighted_matrices(self): \n",
    "    #initialize the cost matrixes for deletions and insertions\n",
    "        insert_costs = np.ones(128, dtype=np.float64)  # make an array of all 1's of size 128, the number of ASCII characters\n",
    "        delete_costs = np.ones (128, dtype=np.float64)\n",
    "\n",
    "        for index,row in self.edits_ins.iterrows(): \n",
    "            insert_costs[ord(index)] = row['transformed_frequency']\n",
    "\n",
    "        for index,row in self.edits_del.iterrows(): \n",
    "            delete_costs[ord(index)] = row['transformed_frequency']\n",
    "\n",
    "        #substitution\n",
    "\n",
    "        substitute_costs = np.ones((128, 128), dtype=np.float64)\n",
    "        lst = []\n",
    "        for index,row in self.edits_sub.iterrows(): \n",
    "            z = tuple([row['edit_from'], row['edit_to'], row['transformed_frequency']])\n",
    "            lst.append (z)\n",
    "        for itm in lst: \n",
    "            itm2 = list(itm)\n",
    "            try: \n",
    "                substitute_costs[ord(itm2[0]), ord(itm2[1])] = itm2[2]\n",
    "            except IndexError: \n",
    "                pass\n",
    "\n",
    "        #transposition\n",
    "\n",
    "        transpose_costs = np.ones((128, 128), dtype=np.float64)\n",
    "\n",
    "        lst = []\n",
    "\n",
    "        for index,row in self.edits_trans.iterrows(): \n",
    "            z = tuple([row['first_letter'], row['second_letter'], row['transformed_frequency']])\n",
    "            lst.append (z)\n",
    "\n",
    "        for itm in lst: \n",
    "            itm2 = list(itm)\n",
    "            try: \n",
    "                transpose_costs[ord(itm2[0]), ord(itm2[1])] = itm2[2]\n",
    "            except IndexError: \n",
    "                print(itm2)\n",
    "\n",
    "        return insert_costs, delete_costs, substitute_costs, transpose_costs\n",
    "\n",
    "    \n",
    "    def weighted_ed_rel (self, cand, token, del_costs, ins_costs, sub_costs, trans_costs): \n",
    "        w_editdist = dam_lev(token, cand, delete_costs = del_costs, insert_costs = ins_costs, \n",
    "                             substitute_costs = sub_costs, transpose_costs = trans_costs)\n",
    "        rel_w_editdist = w_editdist/len(token)\n",
    "        return rel_w_editdist\n",
    "\n",
    "    def run_low (self, word, voc, func, del_costs, ins_costs, sub_costs, trans_costs): \n",
    "        replacement = [' ',100]\n",
    "        for token in voc: \n",
    "            sim = func(word, token, del_costs, ins_costs, sub_costs, trans_costs)\n",
    "            if sim < replacement[1]:\n",
    "                replacement[1] = sim\n",
    "                replacement[0] = token\n",
    "\n",
    "        return replacement   \n",
    "    \n",
    "    \n",
    "    def spelling_correction (self, post, token_freq_dict, token_freq_ordered, min_rel_freq = 2, max_rel_edit_dist = 0.08): \n",
    "        post2 = []\n",
    "        cnt = 0 \n",
    "\n",
    "        for a, token in enumerate (post): \n",
    "            if self.TRUE_WORD.fullmatch(token):\n",
    "                if token in self.spelling_corrections:\n",
    "                    correct = self.spelling_corrections[token] \n",
    "                    post2.append(correct)\n",
    "                    cnt +=1\n",
    "                    self.replaced.append(token)\n",
    "                    self.replaced_with.append(correct)\n",
    "\n",
    "                elif token in self.celex_freq_dict:\n",
    "                    post2.append(token)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    # make the subset of possible candidates\n",
    "                    freq_word = token_freq_dict[token]\n",
    "                    limit = freq_word * min_rel_freq\n",
    "                    subset = [t[0] for t in token_freq_ordered if t[1]>= limit]\n",
    "\n",
    "                    #compare these candidates with the word        \n",
    "                    candidate = self.run_low (token, subset, self.weighted_ed_rel, self.delete_costs_nw, self.insert_costs_nw, \n",
    "                                         self.substitute_costs_nw, self.transpose_costs_nw)\n",
    "\n",
    "                #if low enough RE - candidate is deemed good\n",
    "                    if candidate[1] <= max_rel_edit_dist:\n",
    "                        post2.append(candidate[0]) \n",
    "                        cnt +=1\n",
    "                        self.replaced.append(token)\n",
    "                        self.replaced_with.append(candidate[0])\n",
    "                        self.spelling_corrections [token] = candidate[0]\n",
    "                    else: \n",
    "                        post2.append(token)\n",
    "            else: post2.append(token)\n",
    "        self.total_cnt.append (cnt)\n",
    "        return post2\n",
    "      \n",
    "    def initialize_files_for_spelling(self): \n",
    "        total_cnt = []\n",
    "        replaced = []\n",
    "        replaced_with = []\n",
    "        spelling_corrections= {}\n",
    "        return total_cnt, replaced, replaced_with, spelling_corrections\n",
    "    \n",
    "    def change_tup_to_list (self, tup): \n",
    "        thelist = list(tup)\n",
    "        return thelist\n",
    "\n",
    "    def create_token_freq (self, data): \n",
    "        flat_data = [item for sublist in data for item in sublist]\n",
    "        self.token_freq = Counter(flat_data)\n",
    "        \n",
    "        token_freq_ordered = self.token_freq.most_common ()\n",
    "        self.token_freq_ordered2 = [self.change_tup_to_list(m) for m in token_freq_ordered]\n",
    "    \n",
    "    def correct_spelling_mistakes(self, data): \n",
    "#         data= self.load_obj ('/data/dirksonar/Project1_lexnorm/spelling_correction/output/', 'gistdata_lemmatised')\n",
    "        self.load_files2()\n",
    "        self.insert_costs_nw, self.delete_costs_nw, self.substitute_costs_nw, self.transpose_costs_nw = self.initialize_weighted_matrices()\n",
    "        self.total_cnt, self.replaced, self.replaced_with, self.spelling_corrections = self.initialize_files_for_spelling()\n",
    "    \n",
    "        self.TRUE_WORD = re.compile('[-a-z]+')  # Only letters and dashes  \n",
    "#         data2 = [word_tokenize(m) for m in data]\n",
    "        self.create_token_freq(data)\n",
    "        out = [self.spelling_correction (m, self.token_freq, self.token_freq_ordered2) for m in data]\n",
    "        return out, self.total_cnt, self.replaced, self.replaced_with, self.spelling_corrections\n",
    "    \n",
    "#--------Overall normalization function--------------------------------------\n",
    "\n",
    "    def normalize (self, posts):\n",
    "        self.load_files ()\n",
    "        posts1 = self.anonymize(posts)\n",
    "        posts2 = [self.lowercase (m) for m in posts1]\n",
    "        posts3 = self.remove_non_english (posts2)\n",
    "        posts3b = [word_tokenize(m) for m in posts3]\n",
    "        posts4 = [self.sarker_normalize(posts3b)]\n",
    "        posts5 = [self.expand_abbr(posts4[0], self.abbr_dict)]\n",
    "        posts6, total_cnt, replaced, replaced_with, spelling_corrections = self.correct_spelling_mistakes(posts5[0])\n",
    "        return posts6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "phony_posts = ['advil is good for relief. I like the colour', 'la vie est belle vous temps fait avec moi', 'Daniela works very hard and now has to have an op', 'lol testing is fun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common 10 languages in the data are:['en', 'fr']\n"
     ]
    }
   ],
   "source": [
    "p2 =Normalizer().normalize(phony_posts)\n",
    "print(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['advil', 'is', 'good', 'for', 'relief'], ['-name-', 'works', 'very', 'hard', 'all', 'yr'], ['lol', 'testing', 'is', 'fun']]\n"
     ]
    }
   ],
   "source": [
    "print(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
