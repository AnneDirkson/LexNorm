{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexical normalization pipeline \n",
    "\n",
    "author - AR Dirkson \n",
    "date - 2-10-2018\n",
    "\n",
    "This pipeline takes raw text data and performs: \n",
    "- Removes URLs, email addresses and personal pronouns\n",
    "- Convert to lower-case\n",
    "- Tokenization with NLTK \n",
    "- British English to American English \n",
    "- Normalization of generic abbreviations and slang \n",
    "- Normalization of domain-specific (patient forum) abbreviations \n",
    "- Spelling correction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the relevant objects should be in obj_lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pickle \n",
    "import re\n",
    "import numpy as np \n",
    "from collections import Counter, defaultdict\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import editdistance\n",
    "import csv \n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import scipy.stats \n",
    "from nltk.corpus import names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer (): \n",
    "        \n",
    "    def __init__(self): \n",
    "        pass\n",
    "        \n",
    "    #to use this function the files need to be sorted in the same folder as the script under /obj_lex/\n",
    "    def load_obj(self, name):\n",
    "        with open('obj_lex/' + name + '.pkl', 'rb') as f:\n",
    "            return pickle.load(f, encoding='latin1')\n",
    "        \n",
    "    def load_files(self): \n",
    "        self.ext_vocab2 = self.load_obj('vocabulary_spelling_unique')\n",
    "        self.abbr_dict = self.load_obj ('abbreviations_dict')\n",
    "        self.celex_freq_dict = self.load_obj ('celex_lwrd_frequencies')\n",
    "        self.celex_list = list(celex_freq_dict.keys())\n",
    "        self.celex_set = set (celex_list)\n",
    "        self.drug_norm_dict = self.load_obj ('drug_normalize_dict')\n",
    "\n",
    "    def change_tup_to_list(self, tup): \n",
    "        thelist = list(tup)\n",
    "        return thelist\n",
    "    \n",
    "    def change_list_to_tup(self,thelist): \n",
    "        tup = tuple(thelist)\n",
    "        return tup\n",
    "    \n",
    "#---------Remove URls, email addresses and personal pronouns ------------------\n",
    "        \n",
    "    def replace_urls(self,list_of_msgs): \n",
    "        list_of_msgs2 = []\n",
    "        for msg in list_of_msgs: \n",
    "            nw_msg = re.sub(\n",
    "        r'\\b' + r'((\\(<{0,1}https|\\(<{0,1}http|\\[<{0,1}https|\\[<{0,1}http|<{0,1}https|<{0,1}http)(:|;| |: )\\/\\/|www.)[\\w\\.\\/#\\?\\=\\+\\;\\,\\&\\%_\\n-]+(\\.[a-z]{2,4}\\]{0,1}\\){0,1}|\\.html\\]{0,1}\\){0,1}|\\/[\\w\\.\\?\\=#\\+\\;\\,\\&\\%_-]+|[\\w\\/\\.\\?\\=#\\+\\;\\,\\&\\%_-]+|[0-9]+#m[0-9]+)+(\\n|\\b|\\s|\\/|\\]|\\)|>)',\n",
    "        '-URL-', msg)\n",
    "            list_of_msgs2.append(nw_msg)\n",
    "        return list_of_msgs2    \n",
    "\n",
    "    def replace_email(self,list_of_msgs): \n",
    "        list_of_msgs2 = []\n",
    "        for msg in list_of_msgs: \n",
    "            nw_msg = re.sub (r\"([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)\", '-EMAIL-', msg) \n",
    "            list_of_msgs2.append(nw_msg)\n",
    "        return list_of_msgs2\n",
    "\n",
    "    def remove_empty (self,list_of_msgs): \n",
    "        empty = []\n",
    "        check_msgs3 =[]\n",
    "        for a, i in enumerate (list_of_msgs): \n",
    "            if len(i) == 0: \n",
    "                print('empty')\n",
    "            else: \n",
    "                check_msgs3.append(i)\n",
    "        return check_msgs3\n",
    "    \n",
    "\n",
    "    def create_names_list (self): \n",
    "        male_names = names.words('male.txt')\n",
    "        female_names = names.words('female.txt')\n",
    "        male_set = set (male_names)\n",
    "        female_set = set (female_names)\n",
    "        names_set = male_set.union(female_set) \n",
    "        names_list = [] \n",
    "        for word in names_set: \n",
    "            if (word != 'ned') & (word != 'Ned'): #ned means no evidence and is an important medical term\n",
    "                word1 = str.lower (word)\n",
    "                names_list.append(word1) #add the lowered words\n",
    "                names_list.append(word) #add the capitalized words\n",
    "        \n",
    "        self.names_list = names_list\n",
    "    \n",
    "    def remove_propernoun_names(self,msg):\n",
    "        try: \n",
    "            nw_msg = [self.change_tup_to_list(token) for token in msg]\n",
    "            for a, token in enumerate (nw_msg):\n",
    "                if (token[0] in self.names_list) and ((token[1] == 'NNP') or (token[1]== 'NNPS')): \n",
    "                    new_token = token[0].replace (token[0], \"-NAME-\")\n",
    "                    nw_msg[a] = [new_token, token[1]]\n",
    "#             nw_msg2 = [self.change_list_to_tup(token) for token in nw_msg]\n",
    "            return nw_msg\n",
    "        except TypeError: \n",
    "            pass\n",
    "    \n",
    "    \n",
    "    def anonymize (self, posts): \n",
    "        posts2 = self.replace_urls (posts)\n",
    "        posts3 = self.replace_email (posts2)\n",
    "        posts4 = self.remove_empty(posts3)\n",
    "        posts5 = [word_tokenize (sent) for sent in posts4]\n",
    "        posts6 = [pos_tag(sent) for sent in posts5]\n",
    "        self.create_names_list()\n",
    "        posts7 = [self.remove_propernoun_names (m) for m in posts6]\n",
    "        posts8 = []\n",
    "        for post in posts7: \n",
    "            tg = [m[0] for m in post]\n",
    "            posts8.append(tg)\n",
    "        return posts8\n",
    "\n",
    "#---------Convert to lowercase ----------------------------------------------------\n",
    "    \n",
    "    def lowercase (self, post):\n",
    "        post1 = []\n",
    "        for word in post: \n",
    "            word1 = word.lower()\n",
    "            post1.append (word1)\n",
    "        return post1\n",
    "\n",
    "#----------- Lexical normalization pipeline (Sarker, 2017) -------------------------------\n",
    "\n",
    "    def loadItems(self):\n",
    "        '''\n",
    "        This is the primary load function.. calls other loader functions as required..\n",
    "        '''    \n",
    "        global english_to_american\n",
    "        global noslang_dict\n",
    "        global IGNORE_LIST_TRAIN\n",
    "        global IGNORE_LIST\n",
    "\n",
    "        english_to_american = {}\n",
    "        lexnorm_oovs = []\n",
    "        IGNORE_LIST_TRAIN = []\n",
    "        IGNORE_LIST = []\n",
    "\n",
    "        english_to_american = self.loadEnglishToAmericanDict()\n",
    "        noslang_dict = self.loadDictionaryData()\n",
    "        for key, value in noslang_dict.items (): \n",
    "            value2 = value.lower ()\n",
    "            value3 = word_tokenize (value2)\n",
    "            noslang_dict[key] = value3\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    def loadEnglishToAmericanDict(self):\n",
    "        etoa = {}\n",
    "\n",
    "        english = open('C:\\\\Users\\\\dirksonar\\\\Documents\\\\Python Scripts\\\\External\\\\Lexical_normalization_files\\\\englishspellings.txt')\n",
    "        american = open('C:\\\\Users\\\\dirksonar\\\\Documents\\\\Python Scripts\\\\External\\\\Lexical_normalization_files\\\\americanspellings.txt')\n",
    "        for line in english:\n",
    "            etoa[line.strip()] = american.readline().strip()\n",
    "        return etoa\n",
    "\n",
    "    def loadDictionaryData(self):\n",
    "        '''\n",
    "        this function loads the various dictionaries which can be used for mapping from oov to iv\n",
    "        '''\n",
    "        n_dict = {}\n",
    "        infile = open('C:\\\\Users\\\\dirksonar\\\\Documents\\\\Python Scripts\\\\External\\\\Lexical_normalization_files\\\\noslang_mod.txt')\n",
    "        for line in infile:\n",
    "            items = line.split(' - ')\n",
    "            if len(items[0]) > 0 and len(items) > 1:\n",
    "                n_dict[items[0].strip()] = items[1].strip()\n",
    "        return n_dict\n",
    "\n",
    "\n",
    "\n",
    "    def preprocessText(self, tokens, IGNORE_LIST, ignore_username=False, ignore_hashtag=False, ignore_repeated_chars=True, eng_to_am=True, ignore_urls=False):\n",
    "        '''\n",
    "        Note the reason it ignores hashtags, @ etc. is because there is a preprocessing technique that is \n",
    "            designed to remove them \n",
    "        '''\n",
    "        normalized_tokens =[]\n",
    "        #print tokens\n",
    "        text_string = ''\n",
    "        # NOTE: if nesting if/else statements, be careful about execution sequence...\n",
    "        for t in tokens:\n",
    "            t_lower = t.strip().lower()\n",
    "            # if the token is not in the IGNORE_LIST, do various transformations (e.g., ignore usernames and hashtags, english to american conversion\n",
    "            # and others..\n",
    "            if t_lower not in IGNORE_LIST:\n",
    "                # ignore usernames '@'\n",
    "                if re.match('@', t) and ignore_username:\n",
    "                    IGNORE_LIST.append(t_lower)\n",
    "                    text_string += t_lower + ' '\n",
    "                #ignore hashtags\n",
    "                elif re.match('#', t_lower) and ignore_hashtag:\n",
    "                    IGNORE_LIST.append(t_lower)\n",
    "                    text_string += t_lower + ' '\n",
    "                #convert english spelling to american spelling\n",
    "                elif t.strip().lower() in english_to_american.keys() and eng_to_am:    \n",
    "                    text_string += english_to_american[t.strip().lower()] + ' '\n",
    "                #URLS\n",
    "                elif re.search('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', t_lower) and ignore_urls:\n",
    "                    IGNORE_LIST.append(t_lower)\n",
    "                    text_string += t_lower + ' '                \n",
    "                elif not ignore_repeated_chars and not re.search(r'[^a-zA-Z]', t_lower):\n",
    "                    # if t_lower only contains alphabetic characters\n",
    "                    t_lower = re.sub(r'([a-z])\\1+', r'\\1\\1', t_lower)\n",
    "                    text_string += t_lower + ' '  \n",
    "                    # print t_lower\n",
    "\n",
    "                # if none of the conditions match, just add the token without any changes..\n",
    "                else:\n",
    "                    text_string += t_lower + ' '\n",
    "            else:  # i.e., if the token is in the ignorelist..\n",
    "                text_string += t_lower + ' '\n",
    "            normalized_tokens = text_string.split()\n",
    "        # print normalized_tokens\n",
    "        return normalized_tokens, IGNORE_LIST\n",
    "\n",
    "\n",
    "    def dictionaryBasedNormalization(self, tokens, I_LIST, M_LIST):\n",
    "        tokens2 =[]\n",
    "        for t in (tokens):\n",
    "            t_lower = t.strip().lower()\n",
    "            if t_lower in noslang_dict.keys() and len(t_lower)>2:\n",
    "                nt = noslang_dict[t_lower]\n",
    "                [tokens2.append(m) for m in nt]\n",
    "\n",
    "                if not t_lower in M_LIST:\n",
    "                    M_LIST.append(t_lower)\n",
    "                if not nt in M_LIST:\n",
    "                    M_LIST.append(nt)\n",
    "            else: \n",
    "                tokens2.append (t)\n",
    "        return tokens2, I_LIST, M_LIST\n",
    "    \n",
    "#----Using the Sarker normalization functions ----------------------------\n",
    "#Step 1 is the English normalization and step 2 is the abbreviation normalization\n",
    "\n",
    "    def normalize_step1(self, tokens, oovoutfile=None):\n",
    "        global IGNORE_LIST\n",
    "        global il\n",
    "        MOD_LIST = []\n",
    "        # Step 1: preprocess the text\n",
    "        normalized_tokens, il = self.preprocessText(tokens, IGNORE_LIST)\n",
    "        return normalized_tokens\n",
    "    \n",
    "    def normalize_step2(self, normalized_tokens, oovoutfile=None): \n",
    "        global IGNORE_LIST\n",
    "        global il\n",
    "        MOD_LIST = []    \n",
    "        ml = MOD_LIST\n",
    "        normalized_tokens, il, ml = self.dictionaryBasedNormalization(normalized_tokens, il, ml)\n",
    "        return normalized_tokens\n",
    "\n",
    "    def sarker_normalize (self,list_of_msgs): \n",
    "        self.loadItems()\n",
    "        msgs_normalized = [self.normalize_step1(m) for m in list_of_msgs]\n",
    "        msgs_normalized2 = [self.normalize_step2(m) for m in msgs_normalized]    \n",
    "        return msgs_normalized2\n",
    "\n",
    "#-------Domain specific abreviation expansion ----------------------------\n",
    "# The list of abbreviations is input as a dictionary with tokenized output  \n",
    "\n",
    "    def domain_specific_abbr (self, tokens, abbr): \n",
    "        post2 = [] \n",
    "        for t in tokens:\n",
    "            if t in abbr.keys(): \n",
    "                nt = abbr[t]\n",
    "                [post2.append(m) for m in nt]\n",
    "            else: \n",
    "                post2.append(t)\n",
    "        return post2\n",
    "\n",
    "    def expand_abbr (self, data, abbr): \n",
    "        data2 = []\n",
    "        for post in data: \n",
    "            post2 = self.domain_specific_abbr (tokens = post, abbr= abbr)\n",
    "            data2.append(post2)\n",
    "        return data2\n",
    "    \n",
    "#-------Spelling correction -------------------------------------------------    \n",
    "\n",
    "    def create_token_freq (self, data): \n",
    "        flat_data = [item for sublist in data for item in sublist]\n",
    "        self.token_freq = Counter(flat_data)\n",
    "    \n",
    "    def flev_impr (self,cand, token): \n",
    "        abs_edit_dist = editdistance.eval(cand, token)\n",
    "        rel_edit_dist = abs_edit_dist / len(token)\n",
    "\n",
    "#         if cand[0] != token[0]: # if first letters are different\n",
    "#              rel_edit_dist = rel_edit_dist+1\n",
    "        return rel_edit_dist\n",
    "\n",
    "    def run_low (self,word, voc, func): \n",
    "        replacement = [' ',100]\n",
    "        for token in voc: \n",
    "            sim = func(word, token)\n",
    "            if sim < replacement[1]:\n",
    "                replacement[1] = sim\n",
    "                replacement[0] = token\n",
    "        return replacement   \n",
    "   \n",
    "\n",
    "    def find_mistakes(self,tokens, min_corpus_freq, max_rel_edit_dist):\n",
    "        TRUE_WORD = re.compile('[-a-z]+')  # Only letters and dashes \n",
    "        output= []\n",
    "        self.create_token_freq (tokens)\n",
    "        \n",
    "        min_corpus_freq = min_corpus_freq * sum(self.token_freq.values())\n",
    "        \n",
    "       \n",
    "        z = self.token_freq.most_common()\n",
    "        z = [self.change_tup_to_list(m) for m in z]\n",
    "\n",
    "        spelling_dict = []\n",
    "        [spelling_dict.append(t) for t in self.celex_list]\n",
    "\n",
    "        spelling_dict_extra = [t[0] for t in z if t[1]>= min_corpus_freq and t[0] not in self.celex_set]\n",
    "        spelling_dict_extra = [t for t in spelling_dict_extra if TRUE_WORD.fullmatch(t)]\n",
    "        [spelling_dict.append(t) for t in spelling_dict_extra]\n",
    "        \n",
    "        spelling_corrections = {}\n",
    "        \n",
    "        for post in tokens:\n",
    "            post2 = []\n",
    "            for a, token in enumerate (post):\n",
    "                if TRUE_WORD.fullmatch(token):\n",
    "                    if token in spelling_corrections:\n",
    "                        correct = spelling_corrections[token] \n",
    "                        post2.append(correct)\n",
    "                    elif token in self.celex_freq_dict: # do not need try here because will not throw error if token is not a key\n",
    "                        post2.append(token)\n",
    "                    elif self.token_freq[token] >= min_corpus_freq: # do not need try here because necessarily every token has to be in here\n",
    "                        post2.append(token)\n",
    "                    else:\n",
    "                        #make a subset of hte voc that has the right first letter and not too much difference in length\n",
    "                        l = len(token)\n",
    "                        f = token[0]\n",
    "                        spelling_dict2 = [t for t in spelling_dict if t[0] == f]\n",
    "                        spelling_dict3 = [t for t in spelling_dict2 if len(t)<= (1.2 *l)] #reduce the search space\n",
    "                        spelling_dict4 = [t for t in spelling_dict3 if len(t)>= (0.8 *l)]\n",
    "\n",
    "                        candidate = self.run_low (token, spelling_dict4, flev_impr)\n",
    "\n",
    "                        if candidate[1] >= max_rel_edit_dist: \n",
    "                            post2.append(token)\n",
    "                        else: \n",
    "                            post2.append(candidate[0])\n",
    "                            spelling_corrections [token] = candidate[0]\n",
    "                else: post2.append(token) \n",
    "            output.append(post2)\n",
    "        return output\n",
    "    \n",
    "#--------Overall normalization function--------------------------------------\n",
    "\n",
    "    def normalize (self, posts):\n",
    "        self.load_files ()\n",
    "        posts1 = self.anonymize(posts)\n",
    "        posts2 = [self.lowercase (m) for m in posts1]\n",
    "        posts4 = [self.sarker_normalize(posts2)]\n",
    "        posts5 = [self.expand_abbr(posts4[0], self.abbr_dict)]\n",
    "        posts6 = self.find_mistakes (posts5[0], min_corpus_freq = 5e-6, max_rel_edit_dist = 0.19)\n",
    "        return posts6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
